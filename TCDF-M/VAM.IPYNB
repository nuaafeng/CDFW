{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变量注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, V, T = x.shape\n",
    "Q = y.unsqueeze(2).repeat(1, T, 1)  \n",
    "K = self.Wk(xt.transpose(1,2))  \n",
    "attn_scores = self.softmax(torch.matmul(K.transpose(1,2),Q)/torch.sqrt(torch.tensor(T+1)))\n",
    "attn_scores = attn_scores.repeat(1, 1, T)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCDF-VAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    This module defines a flexible Temporal Convolutional Network (TCN) class that can be configured with different\n",
    "    variants of TCN models, including weight sharing and recurrent architectures.\n",
    "\n",
    "    The TCN class allows for easy instantiation of TCN models based on the desired configuration. It supports the\n",
    "    following TCN variants:\n",
    "\n",
    "    - DefaultTCN: The basic TCN model without weight sharing or recurrence.\n",
    "    - WeightSharingTCN: TCN model with weight sharing across the temporal layers.\n",
    "    - RecurrentTCN: TCN model with recurrent temporal layers.\n",
    "    - WeightSharingRecurrentTCN: TCN model with both weight sharing and recurrent temporal layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, hidden_dim: int, kernel_size: int = 2,\n",
    "                 n_blocks: int = 2, n_layers: int = 2, groups: int = 1,\n",
    "                 dropout: float = 0.0, weight_sharing: bool = False, recurrent: bool = False,\n",
    "                 use_padding: bool = False, use_positional_embedding=False, **kwargs):\n",
    "        \"\"\"\n",
    "        This module defines a flexible Temporal Convolutional Network (TCN) class that can be configured with different\n",
    "        variants of TCN models, including weight sharing and recurrent architectures.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            hidden_dim (int): Hidden dimension of the model.\n",
    "            kernel_size (int): Kernel size for the convolutional layers.\n",
    "            n_blocks (int, optional): Number of blocks in the TCN (default: 2).\n",
    "            n_layers (int, optional): Number of layers per block in the TCN (default: 2).\n",
    "            groups (int, optional): Number of groups for each Conv1d layer (default: 1).\n",
    "            dropout (float, optional): Dropout probability for the convolutional layers (default: 0.0).\n",
    "            weight_sharing (bool, optional): Whether to use weight sharing in the TCN (default: False).\n",
    "            recurrent (bool, optional): Whether to use recurrent temporal layers (default: False).\n",
    "            use_padding (bool, optional): Whether to use padding in convolutional layers to get the same\n",
    "                                          input and output size (default: False).\n",
    "            use_positional_embedding (bool, optional): Whether to use a positional embedding in convolutional layers\n",
    "                                                   for temporal awareness in contemporaneous systems (default: False).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Prepare a dictionary to collect the arguments\n",
    "        config = {\n",
    "            'in_channels': in_channels,\n",
    "            'out_channels': out_channels,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'kernel_size': kernel_size,\n",
    "            'n_blocks': n_blocks,\n",
    "            'n_layers': n_layers,\n",
    "            'groups': groups,\n",
    "            'dropout': dropout,\n",
    "            'use_padding': use_padding,\n",
    "            'use_positional_embedding': use_positional_embedding\n",
    "        }\n",
    "\n",
    "        # Create the correct variant of the TCN based on the flags\n",
    "        if weight_sharing and recurrent:\n",
    "            self.tcn = WeightSharingRecurrentTCN(**config)\n",
    "        elif weight_sharing:\n",
    "            self.tcn = WeightSharingTCN(**config)\n",
    "        elif recurrent:\n",
    "            self.tcn = RecurrentTCN(**config)\n",
    "        else:\n",
    "            self.tcn = DefaultTCN(**config)\n",
    "\n",
    "        self.receptive_field = (2 ** n_blocks - 1) * n_layers * (kernel_size - 1) + 1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the TCN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through the TCN model.\n",
    "        \"\"\"\n",
    "        return self.tcn(self.dropout(x))\n",
    "\n",
    "\n",
    "class DefaultTCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Temporal Convolutional Network (TCN) model that consists of a stack of Temporal Blocks.\n",
    "    The TCN is designed to take in temporal sequences of data and produce a prediction for each time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, hidden_dim: int,\n",
    "                 kernel_size: int, n_blocks: int = 2, n_layers: int = 1,\n",
    "                 groups: int = 1, dropout: float = 0.0, use_padding: bool = False,\n",
    "                 use_positional_embedding: bool = False):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % groups == 0, \"'hidden_dim' should be a multiple of 'groups'\"\n",
    "\n",
    "        # Calculate the total number of layers in the TCN\n",
    "        self.n_total_layers = n_blocks * n_layers + 1\n",
    "\n",
    "        # Initialize the activation and dropout layers\n",
    "        relu = nn.ReLU()\n",
    "        dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # Create the stack of Temporal Blocks\n",
    "        modules = []\n",
    "        for i in range(n_blocks):\n",
    "            modules.append(\n",
    "                TemporalBlock(\n",
    "                    in_channels=in_channels if i == 0 else hidden_dim,\n",
    "                    out_channels=hidden_dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=2 ** i,\n",
    "                    groups=groups,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=dropout,\n",
    "                    use_residual=i > 0,\n",
    "                    use_padding=use_padding,\n",
    "                    use_positional_embedding=i == 0 and use_positional_embedding)\n",
    "            )\n",
    "            modules.append(relu)\n",
    "            modules.append(dropout_layer)\n",
    "\n",
    "        # Create the final prediction layer\n",
    "        modules.append(\n",
    "            nn.Conv1d(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                groups=groups\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.network = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class RecurrentTCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Recurrent Temporal Convolutional Network (Rec-TCN) model that consists of a first Temporal Block\n",
    "    followed by a series of Recurrent Blocks. The Rec-TCN is designed to take in temporal sequences of data\n",
    "    and produce a prediction for each time step.\n",
    "\n",
    "    This version of the Rec-TCN has much fewer number of parameters compared to a general TCN when the number\n",
    "    of layers is large.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, hidden_dim: int,\n",
    "                 kernel_size: int, n_blocks: int = 3, n_layers: int = 1,\n",
    "                 groups: int = 1, dropout: float = 0.0, use_padding: bool = False,\n",
    "                 use_positional_embedding: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_blocks = n_blocks\n",
    "\n",
    "        assert self.n_blocks >= 3, 'if n_blocks is smaller than 3, please use a regular TCN'\n",
    "\n",
    "        # Define the first TCN block\n",
    "        self.first_block = TemporalBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=1,\n",
    "            groups=groups,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            use_residual=False,\n",
    "            use_padding=use_padding,\n",
    "            use_positional_embedding=use_positional_embedding\n",
    "        )\n",
    "\n",
    "        # Define the recurrent block\n",
    "        self.recurrent = TemporalBlock(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=1,\n",
    "            groups=groups,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            use_padding=use_padding\n",
    "        )\n",
    "\n",
    "        # Define the dropout and ReLU layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Create the final prediction layer\n",
    "        self.predictions = nn.Conv1d(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            groups=groups\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input tensor through the first TCN block\n",
    "        x = self.first_block(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass input tensor through the residual blocks\n",
    "        for i in range(self.n_blocks - 1):\n",
    "            # Update the dilation factor for the Recurrent Block\n",
    "            self.recurrent.dilation = 2 ** (i + 1)\n",
    "\n",
    "            # Pass input tensor through the Recurrent Block\n",
    "            x = self.recurrent(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Pass the output through the final prediction layer\n",
    "        return self.predictions(x)\n",
    "\n",
    "\n",
    "class WeightSharingTCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Weight Sharing Temporal Convolutional Network (WS-TCN) implementation. Weight sharing reduces\n",
    "    the number of parameters and makes the model more efficient. Positional encoding provides information\n",
    "    about the position of each element in the input sequence. Recurrent option can be used for even less parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, hidden_dim: int,\n",
    "                 kernel_size: int, n_blocks: int = 3, n_layers: int = 1,\n",
    "                 groups: int = 1, dropout: float = 0.0, use_padding: bool = False,\n",
    "                 use_positional_embedding: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % groups == 0, \"The hidden dimension must be divisible by the number of groups\"\n",
    "        assert n_blocks >= 2, 'if n_blocks is smaller than 2, please use a regular TCN'\n",
    "\n",
    "        self.groups = groups\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize the activation and dropout layers\n",
    "        relu = nn.ReLU()\n",
    "        dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # Define the first TCN block\n",
    "        self.first_block = nn.Sequential(TemporalBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=1,\n",
    "            groups=groups,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            use_residual=False,\n",
    "            use_padding=use_padding,\n",
    "            use_positional_embedding=use_positional_embedding\n",
    "        ), relu, dropout_layer)\n",
    "\n",
    "        # Create the stack of Temporal Blocks\n",
    "        modules = []\n",
    "        for i in range(1, n_blocks):\n",
    "            modules.append(\n",
    "                TemporalBlock(\n",
    "                    in_channels=hidden_dim // groups,\n",
    "                    out_channels=hidden_dim // groups,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=2 ** i,\n",
    "                    groups=1,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=dropout,\n",
    "                    use_padding=use_padding)\n",
    "            )\n",
    "            modules.append(relu)\n",
    "            modules.append(dropout_layer)\n",
    "\n",
    "        self.weight_shared = nn.Sequential(*modules)\n",
    "\n",
    "        # Create the final prediction layer\n",
    "        self.prediction = nn.Conv1d(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                groups=groups\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.first_block(x)  # (batch_size, hidden_dim, seq_len)\n",
    "\n",
    "        # Reshape input tensor to match the expected shape of the TCN\n",
    "        # (batch_size * groups, hidden_dim // groups, seq_len)\n",
    "        x = x.reshape(-1, self.hidden_dim // self.groups, x.size(-1))\n",
    "\n",
    "        # Apply the TCN to the input tensor\n",
    "        # (batch_size * groups, out_channels // groups, seq_len)\n",
    "        x = self.weight_shared(x)\n",
    "\n",
    "        # Reshape the output tensor to match the expected shape of the WS-TCN\n",
    "        x = x.reshape(batch_size, -1, x.size(-1))  # (batch_size, out_channels, seq_len)\n",
    "\n",
    "        # make the prediction\n",
    "        x = self.prediction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class WeightSharingRecurrentTCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Recurrent Temporal Convolutional Network (Rec-TCN) model that consists of a first Temporal Block\n",
    "    followed by a series of Recurrent Blocks. The Rec-TCN is designed to take in temporal sequences of data\n",
    "    and produce a prediction for each time step.\n",
    "\n",
    "    This version of the Rec-TCN has much fewer number of parameters compared to a general TCN when the number\n",
    "    of layers is large.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, hidden_dim: int,\n",
    "                 kernel_size: int, n_blocks: int = 3, n_layers: int = 1,\n",
    "                 groups: int = 1, dropout: float = 0.0, use_padding: bool = False,\n",
    "                 use_positional_embedding: bool = False):\n",
    "        super().__init__()\n",
    "        assert n_blocks >= 3, 'if n_blocks is smaller than 3, please use a regular WS-TCN'\n",
    "        self.groups = groups\n",
    "        self.n_blocks = n_blocks\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define the dropout and ReLU layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Define the first TCN block\n",
    "        self.first_block = TemporalBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=1,\n",
    "            groups=groups,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            use_residual=False,\n",
    "            use_padding=use_padding,\n",
    "            use_positional_embedding=use_positional_embedding\n",
    "        )\n",
    "\n",
    "        # Define the recurrent block\n",
    "        self.recurrent_weight_shared = TemporalBlock(\n",
    "            in_channels=hidden_dim // groups,\n",
    "            out_channels=hidden_dim // groups,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=1,\n",
    "            groups=1,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            use_padding=use_padding\n",
    "        )\n",
    "\n",
    "        # Create the final prediction layer\n",
    "        self.predictions = nn.Conv1d(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            groups=groups\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Pass input tensor through the first TCN block\n",
    "        x = self.first_block(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Reshape input tensor to match the expected shape of the TCN\n",
    "        # (batch_size * groups, hidden_dim // groups, seq_len)\n",
    "        x = x.reshape(-1, self.hidden_dim // self.groups, x.size(-1))\n",
    "\n",
    "        # Pass input tensor through the residual blocks\n",
    "        for i in range(self.n_blocks - 1):\n",
    "            # Update the dilation factor for the Recurrent Block\n",
    "            self.recurrent_weight_shared.dilation = 2 ** (i + 1)\n",
    "\n",
    "            # Pass input tensor through the Recurrent Block\n",
    "            x = self.recurrent_weight_shared(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Reshape the output tensor to match the expected shape of the WS-TCN\n",
    "        x = x.reshape(batch_size, -1, x.size(-1))  # (batch_size, out_channels, seq_len)\n",
    "\n",
    "        # Pass the output through the final prediction layer\n",
    "        return self.predictions(x)\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module that implements a Temporal Block.\n",
    "\n",
    "    This module consists of a sequence of 1D convolutional layers with dilations, followed by ReLU activations\n",
    "    and dropout layers. The number of layers, kernel size, dilation, and dropout rate can be configured\n",
    "    during initialization. Additionally, it allows external configuration of the 'dilation' attribute,\n",
    "    facilitating its use in a recurrent context (necessary for RecurrentTCN).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        kernel_size (int): Size of the convolutional kernel.\n",
    "        dilation (int, optional): Dilation factor for the convolutional layers (default: 1).\n",
    "        groups (int, optional): Number of groups to split the input and output channels into (default: 1).\n",
    "        n_layers (int, optional): Number of convolutional layers in the block (default: 1).\n",
    "        dropout (float, optional): Dropout rate to use between layers (default: 0.0).\n",
    "        use_padding (bool, optional): Whether to use padding in convolutional layers (default: False).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int,\n",
    "                 dilation: int = 1, groups: int = 1, n_layers: int = 1, dropout: float = 0.0,\n",
    "                 use_residual=True, use_padding: bool = False, use_positional_embedding: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert n_layers > 0, \"Number of layers should be 1 or greater.\"\n",
    "\n",
    "        # Set class variables\n",
    "        self.n_layers = n_layers\n",
    "        self.dilation = dilation\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Define dropout, and activation functions\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Define the network architecture\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        # Create down-sample branch if input and output channels differ\n",
    "        self.down_sample = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            groups=groups\n",
    "        ) if in_channels != out_channels and use_residual else None\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.use_padding = use_padding\n",
    "        self.positional_embedding = None\n",
    "        if use_positional_embedding:\n",
    "            self.positional_embedding = PositionalEmbedding(out_channels, groups=groups)\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            self.convolutions.append(nn.utils.parametrizations.weight_norm(nn.Conv1d(\n",
    "                in_channels=in_channels if i == 0 else out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                groups=groups\n",
    "            )))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Temporal Block to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_channels, new_sequence_length).\n",
    "        \"\"\"\n",
    "        # Set correct dilations\n",
    "        for conv in self.convolutions:\n",
    "            conv.dilation = self.dilation\n",
    "\n",
    "        # Save input tensor to be used as identity\n",
    "        identity = x\n",
    "\n",
    "        # If necessary, pass input tensor through down-sample branch\n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(identity)\n",
    "\n",
    "        for i in range(self.n_layers - 1):\n",
    "            # Apply the layer\n",
    "            if self.use_padding:\n",
    "                x = nn.functional.pad(x, (self.dilation * (self.kernel_size - 1), 0), 'constant', 0)\n",
    "            x = self.convolutions[i](x)\n",
    "            x = self.relu(x)\n",
    "            if i == 0 and self.positional_embedding is not None:\n",
    "                x = self.positional_embedding(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Do not apply relu/dropout to last layer\n",
    "        if self.use_padding:\n",
    "            x = nn.functional.pad(x, (self.dilation * (self.kernel_size - 1), 0), 'constant', 0)\n",
    "        x = self.convolutions[-1](x)\n",
    "\n",
    "        # Add residual connection to the data\n",
    "        if self.use_residual:\n",
    "            x = x + identity[..., -x.size(-1):]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_dim, groups=1, max_length=3000):\n",
    "        super().__init__()\n",
    "        h = hidden_dim // groups\n",
    "        # Create the positional embedding\n",
    "        pos_embedding = torch.zeros(1, h, max_length)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, h, 2).float() * (-math.log(10000.0) / h))\n",
    "        pos_embedding[0, 0::2, :] = torch.sin(position * div_term).t()\n",
    "        pos_embedding[0, 1::2, :] = torch.cos(position * div_term).t()\n",
    "        pos_embedding *= 0.5\n",
    "        pos_embedding = pos_embedding.repeat(1, groups, 1)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding[..., :x.size(-1)]\n",
    "\n",
    "\n",
    "class TAMCaD_UA(nn.Module):\n",
    "    def __init__(self, n_variables, hidden_dim, lambda1, gamma, n_ensembles, use_gumbel, p=0.5, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma  # continuous matrices\n",
    "        self.lambda1 = lambda1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_variables = n_variables\n",
    "        self.n_ensembles = n_ensembles\n",
    "        self.groups = n_variables * n_ensembles\n",
    "\n",
    "        if n_ensembles > 1:\n",
    "            self.register_buffer('bernoulli_mask', torch.rand(1, n_ensembles, n_variables, n_variables, 1) < p)\n",
    "\n",
    "        self.tcn = TCN(\n",
    "                in_channels=self.groups,\n",
    "                out_channels=self.groups * (hidden_dim + 2 * n_variables),\n",
    "                hidden_dim=self.groups * hidden_dim,\n",
    "                groups=self.groups,\n",
    "                **kwargs\n",
    "        )\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.groups * hidden_dim,\n",
    "                      out_channels=self.groups * (hidden_dim // 2),\n",
    "                      kernel_size=1, groups=self.groups),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.groups * (hidden_dim // 2),\n",
    "                      out_channels=self.groups,\n",
    "                      kernel_size=1, groups=self.groups)\n",
    "        )\n",
    "        self.softmax = GumbelSoftmax(temperature=0.9) if use_gumbel else SoftmaxModule()\n",
    "\n",
    "    def forward(self, x, x_noise_adjusted=None, create_artifacts=False, ground_truth=None, mask=None, temporal_matrix=False):\n",
    "\n",
    "        batch_size, n_var, seq_len = x.size()\n",
    "\n",
    "        context, attn_mean, attn_var = self.tcn(x.repeat(1, self.n_ensembles, 1)) \\\n",
    "            .reshape(batch_size, self.n_ensembles, n_var, 2 * n_var + self.hidden_dim, -1) \\\n",
    "            .split([self.hidden_dim, n_var, n_var], dim=-2)\n",
    "\n",
    "        s = context.size(-1)\n",
    "\n",
    "        attn_var = nn.functional.softplus(attn_var) + 1e-6\n",
    "        attention_logits = attn_mean\n",
    "        if self.training:\n",
    "            attention_logits = attn_mean + torch.randn_like(attn_var) * attn_var\n",
    "\n",
    "        # Apply masking if provided\n",
    "        if mask is None:\n",
    "            attentions = self.softmax(attention_logits, dim=-2)\n",
    "        else:\n",
    "            attentions = self.softmax(torch.masked_fill(attention_logits, mask, -1e9), dim=-2)\n",
    "\n",
    "        # x: (batch_size, n_var * hidden_dim, sequence_length)\n",
    "        z = torch.einsum('beijt, bejdt -> beidt', attentions, context) \\\n",
    "            .reshape(batch_size, -1, s)\n",
    "\n",
    "        prediction = self.prediction(z).reshape(-1, self.n_ensembles, n_var, s)  # (bs, n_ensembles, n_var, seq_len)\n",
    "\n",
    "        return self.process(x, prediction, attentions, attn_mean, attn_var, x_noise_adjusted,\n",
    "                            create_artifacts, ground_truth, temporal_matrix)\n",
    "\n",
    "    def process(self, x, prediction, attentions, attention_logits, attention_logits_var,\n",
    "                x_noise_adjusted, create_artifacts, ground_truth, temporal_matrix):\n",
    "        \"\"\"\n",
    "        Processes the outputs of the forward pass, computing losses and other metrics.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Original input tensor of size (batch_size, n_var, seq_len).\n",
    "            prediction (torch.Tensor): Prediction tensor from the model\n",
    "                of size (batch_size, n_ensembles, n_var, seq_len).\n",
    "            attentions (torch.Tensor): attentions tensor from the model\n",
    "                of size (batch_size, n_var, n_ensembles, n_var, seq_len).\n",
    "            attention_logits (torch.Tensor): attention_logits tensor from the model\n",
    "                of size (batch_size, n_var, n_ensembles, n_var, seq_len).\n",
    "            attention_logits_var (torch.Tensor): Var Contributions tensor from the model\n",
    "                of size (batch_size, n_var, n_ensembles, n_var, seq_len).\n",
    "            x_noise_adjusted (torch.Tensor, optional): Tensor of true mean values of the time series\n",
    "                of size (batch_size, n_var, seq_len).\n",
    "            create_artifacts (bool): Flag indicating whether to return the artifacts.\n",
    "            ground_truth (torch.Tensor, optional): Ground truth tensor for the causal matrix is of size (n_var, n_var).\n",
    "            temporal_matrix (bool): return causal matrix with time axis.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the loss, prediction, and optional metrics like causal matrix and AUROC.\n",
    "        \"\"\"\n",
    "        metrics, artifacts = {}, {}\n",
    "\n",
    "        s = attention_logits.size(-1)\n",
    "        regression_loss = (x[:, None, :, 1 - s:] - prediction[..., :-1]).pow(2).mean()\n",
    "        prediction = prediction.detach().mean(dim=1)  # (1, n_var, seq_len)\n",
    "\n",
    "        regularization = self.gamma * torch.diff(attention_logits, dim=-1).abs().mean()\n",
    "        if self.n_ensembles > 1:\n",
    "            mask_loss = torch.masked_fill(attentions, self.bernoulli_mask, value=0).mean()\n",
    "            regularization = regularization + self.lambda1 * mask_loss\n",
    "\n",
    "        # to size (n_ensembles, n_var, n_var, seq_len)\n",
    "        attention_logits = attention_logits.detach().squeeze(0)  # (n_ens, n_var, n_var, seq_len)\n",
    "        attention_logits_var = attention_logits_var.detach().squeeze(0)  # (n_ens, n_var, n_var, seq_len)\n",
    "\n",
    "        metrics['loss'] = regression_loss + regularization\n",
    "        if create_artifacts:\n",
    "            artifacts = {\n",
    "                'prediction': prediction.squeeze(0)\n",
    "            }\n",
    "\n",
    "        # Additional computations if noise-adjusted values are provided\n",
    "        if x_noise_adjusted is not None:\n",
    "            metrics['noise_adjusted_regression_loss'] = nn.functional.mse_loss(x_noise_adjusted[..., 1 - s:],\n",
    "                                                                               prediction[..., :-1])\n",
    "\n",
    "        # Compute causal matrix and AUROC if needed\n",
    "        if create_artifacts or ground_truth is not None:\n",
    "\n",
    "            attn_logits_mean = attention_logits.mean(dim=0)  # (n_var, n_var, seq_len)\n",
    "            attn_logits_ep = attention_logits.std(dim=0)  # (n_var, n_var, seq_len)\n",
    "            attn_logits_al = attention_logits_var.mean(dim=0)  # (n_var, n_var, seq_len)\n",
    "\n",
    "            matrix_mean = attn_logits_mean.mean(dim=-1)  # (n_var, n_var)\n",
    "            matrix_ep = attn_logits_ep.mean(dim=-1)  # (n_var, n_var)\n",
    "            matrix_al = attn_logits_al.mean(dim=-1)  # (n_var, n_var)\n",
    "\n",
    "            matrix_mean = min_max_normalization(matrix_mean, min_val=0.0, max_val=1.0)\n",
    "            matrix_temporal = min_max_normalization(attn_logits_mean, min_val=0.0, max_val=1.0)\n",
    "            if create_artifacts:\n",
    "                artifacts.update({\n",
    "                    'attention_logits': attn_logits_mean,\n",
    "                    'attention_logits_ep': attn_logits_ep,\n",
    "                    'attention_logits_al': attn_logits_al,\n",
    "                    'matrix': matrix_mean,\n",
    "                    'matrix_ep': matrix_ep,\n",
    "                    'matrix_al': matrix_al,\n",
    "                    'matrix_temporal': matrix_temporal\n",
    "                })\n",
    "\n",
    "            if ground_truth is not None:\n",
    "                eval_matrix = matrix_mean\n",
    "                eval_matrix_ep = matrix_ep\n",
    "                if temporal_matrix:\n",
    "                    eval_matrix = matrix_temporal[..., :-1]\n",
    "                    eval_matrix_ep = attn_logits_ep[..., :-1]\n",
    "                    ground_truth = ground_truth[..., 1 - s:]\n",
    "                ground_truth = ground_truth.to(eval_matrix.device)\n",
    "                #ground_truth = ground_truth.unsqueeze(0).repeat(self.n_ensembles, 1, 1)\n",
    "                auc, tpr, fpr = AUROC(ground_truth, eval_matrix)\n",
    "                soft_auc, soft_tpr, soft_fpr = soft_AUROC(ground_truth, eval_matrix, eval_matrix_ep)\n",
    "\n",
    "                metrics.update({'AUROC': auc, 'soft_AUROC': soft_auc})\n",
    "                if create_artifacts:\n",
    "                    artifacts.update({'TPR': tpr, 'FPR': fpr, 'soft_TPR': soft_tpr, 'soft_FPR': soft_fpr})\n",
    "\n",
    "        return metrics, artifacts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
